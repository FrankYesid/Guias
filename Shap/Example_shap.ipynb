{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de cómo utilizar SHAP (SHapley Additive exPlanations) para interpretar un modelo de clasificación después de entrenarlo:\n",
    "\"\"\"https://christophm.github.io/interpretable-ml-book/shap.html#shap-feature-importance\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define features and target \n",
    "X,y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Split the dataset into 75% for training and 25% for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = XGBClassifier(random_state=42) \n",
    "model.fit(X_train, y_train) \n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the model into the TreeExplainer function of shap \n",
    "import shap \n",
    "explainer = shap.TreeExplainer(model) \n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names \n",
    "feature_names = ['Pregnancies', 'Glucose', 'BloodPressure','SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module \n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDP for a single feature \n",
    "pdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=feature_names, feature='Glucose') \n",
    "pdp.pdp_plot(pdp_goals, 'Glucose') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pdp_interact() function \n",
    "interaction = pdp.pdp_interact(model=model, dataset=X_test, model_features=feature_names, features=['Age','BMI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph \n",
    "pdp.pdp_interact_plot(pdp_interact_out=interaction, feature_names=['Age','BMI'], plot_type='contour', plot_pdp=True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the model and test dataset\n",
    "my_set = PermutationImportance(model, random_state=34).fit(X_test,y_test)\n",
    "eli5.show_weights(my_set, feature_names = X_test.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME (Explicaciones Locales Interpretables Modelo-Agnósticas) se utiliza ampliamente para explicar modelos de caja negra a nivel local. Cuando tenemos modelos complejos como CNN, LIME utiliza un modelo simple y explicativo para comprender su predicción. Para hacerlo aún más fácil de entender, veamos cómo funciona LIME paso a paso:\n",
    "\n",
    "Definir tu punto local: Elige una predicción específica que desees explicar (por ejemplo, por qué una imagen fue clasificada como un gato por una CNN).\n",
    "Generar variaciones: Crea ligeras variaciones en los datos de entrada (por ejemplo, píxeles ligeramente modificados en la imagen).\n",
    "Predecir con el modelo original: Pasa la entrada a la CNN y obtén la clase de salida predicha para cada variación.\n",
    "Construir un modelo explicativo: Entrena un modelo lineal simple para explicar la relación entre las variaciones y las predicciones del modelo.\n",
    "Interpretar el modelo explicativo: Ahora, puedes interpretar el modelo explicativo con cualquier método como importancia de características, PDP, etc. para entender qué características jugaron un papel crucial en la predicción original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaciones del mundo real\n",
    "La Inteligencia Artificial Explicable es el puente que construye la confianza entre el mundo de la tecnología y los humanos. Veamos algunos ejemplos poderosos de Inteligencia Artificial Explicable en nuestro mundo cotidiano:\n",
    "\n",
    "Prácticas justas de préstamos: La Inteligencia Artificial Explicable (XAI) puede proporcionar a los bancos explicaciones claras para las negaciones de préstamos. Las empresas pueden estar libres de riesgos de cumplimiento y también mejorar la confianza de su base de clientes.\n",
    "\n",
    "Eliminar sesgos en la contratación: Muchas empresas utilizan sistemas de IA para filtrar inicialmente un gran número de solicitudes de empleo. Las herramientas XAI pueden revelar cualquier sesgo incrustado en los algoritmos de contratación impulsados por IA. Esto garantiza prácticas de contratación justas basadas en el mérito, no en sesgos ocultos.\n",
    "\n",
    "Aumentar la adopción de vehículos autónomos: ¿Cuántos de ustedes confiarían en un coche sin conductor hoy? La XAI puede explicar el proceso de toma de decisiones de los coches autónomos en la carretera, como cambios de carril o maniobras de emergencia. Esto mejorará la confianza de los pasajeros.\n",
    "\n",
    "Mejorar el diagnóstico médico: La XAI puede proporcionar transparencia en el proceso de diagnóstico al ofrecer una explicación post hoc de las salidas del modelo o diagnósticos. Esto permite a los profesionales médicos obtener una visión más holística del caso del paciente en cuestión.\n",
    "\n",
    "Encuentra un ejemplo que involucre el diagnóstico de una infección por COVID-19 en la imagen a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
